{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64523f3f-7178-4382-8b8e-097ae155f579",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "Based on [Geeks for geeks tutorial](https://www.geeksforgeeks.org/q-learning-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034908d0-d0eb-41aa-b599-6aad6cf5c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "n_states = 16  # Number of states in the grid world\n",
    "n_actions = 4  # Number of possible actions (up, down, left, right)\n",
    "goal_state = 15  # Goal state\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Define parameters\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "exploration_prob = 0.2\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "257d3edd-5190-44dc-bb52-3c8d7fed76d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 14 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, n_actions)  \u001b[38;5;66;03m# Explore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mQ_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Exploit\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Simulate the environment (move to the next state)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# For simplicity, move to the next state\u001b[39;00m\n\u001b[1;32m     14\u001b[0m next_state \u001b[38;5;241m=\u001b[39m (current_state \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m n_states\n",
      "\u001b[0;31mIndexError\u001b[0m: index 14 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm\n",
    "for epoch in range(epochs):\n",
    "    current_state = np.random.randint(0, n_states)  # Start from a random state\n",
    "\n",
    "    while current_state != goal_state:\n",
    "        # Choose action with epsilon-greedy strategy\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = np.random.randint(0, n_actions)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state])  # Exploit\n",
    "\n",
    "        # Simulate the environment (move to the next state)\n",
    "        # For simplicity, move to the next state\n",
    "        next_state = (current_state + 1) % n_states\n",
    "\n",
    "        # Define a simple reward function (1 if the goal state is reached, 0 otherwise)\n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "\n",
    "        # Update Q-value using the Q-learning update rule\n",
    "        Q_table[current_state, action] += learning_rate * \\\n",
    "            (reward + discount_factor *\n",
    "             np.max(Q_table[next_state]) - Q_table[current_state, action])\n",
    "\n",
    "        current_state = next_state  # Move to the next state\n",
    "\n",
    "# After training, the Q-table represents the learned Q-values\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66cc3eb-5399-4fc1-a942-fd02376a8e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59873694, 0.59873694, 0.59873694, 0.59873693])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d510c87-0d34-472a-971e-c55c671903fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_matrix = np.array([[0, -1, -1, 0],\n",
    "                     [-1, 100, 0, -1],\n",
    "                     [0, 100, -1, -1],\n",
    "                     [-1, -1, 0, 0]])\n",
    "Q_table = R_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6a30c-7566-49d6-90ac-71456d009b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
